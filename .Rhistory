install.packages("RColorBrewer")
blogsText <- Corpus(VectorSource(blogs))
blogsText
blogsText <- tm_map(blogsText, stripWhitespace)
blogsText
blogsText <- tm_map(blogsText, tolower)
blogsText <- VCorpus(VectorSource(blogs))
View(blogsText)
View(blogsSample)
blogsText[[1]]
blogsText[[2]]
library(testthat)
#library(tokenizers)
library(stopwords)
#library(sentimentr)
library(dplyr)
library(corpus)
library(RWeka)
library(tm)
set.seed(43)
# Getting and cleaning data
## Data Load
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
## Loading Data
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'))
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'))
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'))
### Sampling
if (!file.exists('./data/sample')){
dir.create('./data/sample')
} else {
print("the directory already exists")
}
blogsSample <- blogs[rbinom(length(blogs)*.1, length(blogs), 0.1)]
newsSample <- blogs[rbinom(length(news)*.1, length(news), 0.1)]
twitterSample <- blogs[rbinom(length(twitter)*.1, length(twitter), 0.1)]
### Write samples into files
writeLines(blogsSample, file = './data/sample/en_US.blogsSample.csv')
writeLines(newsSample, file = './data/sample/en_US.newsSample.csv')
writeLines(twitterSample, file = './data/sample/en_US.twitterSample.csv')
writeLines(blogsSample, con = './data/sample/en_US.blogsSample.txt')
library(testthat)
#library(tokenizers)
library(stopwords)
#library(sentimentr)
library(dplyr)
library(corpus)
library(RWeka)
library(tm)
set.seed(43)
# Getting and cleaning data
## Data Load
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
## Loading Data
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'))
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'))
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'))
### Sampling
if (!file.exists('./data/sample')){
dir.create('./data/sample')
} else {
print("the directory already exists")
}
blogsSample <- blogs[rbinom(length(blogs)*.1, length(blogs), 0.1)]
newsSample <- blogs[rbinom(length(news)*.1, length(news), 0.1)]
twitterSample <- blogs[rbinom(length(twitter)*.1, length(twitter), 0.1)]
### Write samples into files
writeLines(blogsSample, con = './data/sample/en_US.blogsSample.txt')
writeLines(newsSample, con = './data/sample/en_US.newsSample.txt')
writeLines(twitterSample, con = './data/sample/en_US.twitterSample.txt')
rm(list = ls())
blogsSample <- readLines(file = './data/sample/en_US.blogsSample.txt')
newsSample <- readLines(file = './data/sample/en_US.newsSample.txt')
twitterSample <- readLines(file = './data/sample/en_US.twitterSample.txt')
blogsSample <- readLines(con = './data/sample/en_US.blogsSample.txt')
newsSample <- readLines(con = './data/sample/en_US.newsSample.txt')
twitterSample <- readLines(con = './data/sample/en_US.twitterSample.txt')
text <- Corpus(VectorSource(blogsSample, newsSample, twitterSample))
texts <- append(texts, blogsSample)
texts <- append(texts, newsSample)
texts <- append(texts, twitterSample)
texts <- append(blogsSample, newsSample)
texts <- append(texts, twitterSample)
docs <- Corpus(VectorSource(texts))
docs <- VCorpus(VectorSource(texts))
docs <- tm_map(docs, stripWhitespace)
View(docs)
View(texts)
library(sentimentr)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, lexicon::profanity_alvarez)
docs <- tm_map(docs, removeWords, unlist(lexicon::profanity_alvarez))
docs <- tm_map(docs, removeWords, unlist(lexicon::profanity_alvarez))
lexicon::profanity_alvarez
docs <- tm_map(docs, removeWords, regexpr(lexicon::profanity_alvarez))
docs <- tm_map(docs, removeWords, profanity)
profanityList <- lexicon::profanity_alvarez
writeLines(profanityList, con = './data/sample/profanityList.txt')
profanityList <- readline(con = './data/sample/profanityList.txt')
profanityList <- readLines(con = './data/sample/profanityList.txt')
texts <- append(blogsSample, newsSample)
texts <- append(texts, twitterSample)
docs <- tm_map(docs, removeWords, profanity)
docs <- tm_map(docs, removeWords, list(profanity))
docs <- tm_map(docs, removeWords, unlist(profanity))
docs <- tm_map(docs, removeWords, profanityList)
docs <- tm_map(docs, removeWords, grepl(profanityList))
docs <- tm_map(docs, removeWords, profanityList)
full <- Corpus(DirSource("data/sample/"), readerControl = list(readPlain,language="en_US", load = TRUE))
rm(list = ls()) # Clean up
library(testthat)
#library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
#library(corpus)
library(RWeka)
library(tm)
set.seed(43)
full <- Corpus(DirSource("data/sample/"), readerControl = list(readPlain,language="en_US", load = TRUE))
View(full)
length(full)
str(full[[1]])
summary(full)
meta(full[[1]])
length(full[[1]]$content)
full <- tm_map(full, stripWhitespace)
full <- tm_map(full, tolower)
full <- tm_map(full, removeNumbers)
full <- tm_map(full, removePunctuation)
words <- Boost_tokenizer(docs)
words <- Boost_tokenizer(full)
View(words)
full <- tm_map(full, removeWords, stopwords(kind = "en"))
full <- tm_map(full, tolower)
full <- tm_map(full, removeNumbers)
full <- tm_map(full, removePunctuation)
full <- tm_map(full, stripWhitespace)
words <- Boost_tokenizer(full)
View(words)
full <- tm_map(full, removeWords, Vector(profanityList))
full <- tm_map(full, removeWords, vector(profanityList))
profanityList <- readLines(con = './data/profanityList.txt')
profanityList <- readLines(con = './data/profanityList.csv')
full <- tm_map(full, removeWords, vector(profanityList))
full <- tm_map(full, removeWords, profanityList)
full <- tm_map(full, removeWords, c(profanityList))
stopwords(kind = "en")
profanityList
kind(profanityList)
stopwords <- stopwords(kind = "en")
full <- tm_map(full, removeWords, profanityList[,1])
full <- tm_map(full, removeWords, profanityList[1])
full <- tm_map(full, removeWords, gregexpr(profanityList))
full <- tm_map(full, removeWords, gregexpr(profanityList[1]))
full <- tm_map(full, removeWords, regexpr(profanityList))
length(full[[1]]$content)
tm_filter(ful, FUN = searchFullText, profanityList, doclevel = TRUE)
tm_filter(full, FUN = searchFullText, profanityList, doclevel = TRUE)
lexicon::profanity_arr_bad
profanityList <- lexicon::profanity_banned
write.table(profanityList, file = './data/profanityList.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
profanityList <- readLines(con = './data/profanityList.csv')
full <- tm_map(full, removeWords, profanityList)
words <- Boost_tokenizer(full)
View(words)
wordsStem <- tm_map(full, stemDocument, language = "english")
View(wordsStem)
wordsStem <- Boost_tokenizer(wordsStem)
View(wordsStem)
wordsStem <- tm_map(full, stemDocument, language = "english")
dtm <- TermDocumentMatrix(wordsStem)
m <- as.matrix(dtm)
m
v <- sort(rowSums(m),decreasing=TRUE)
v
str(v)
View(v)
hist(words)
hist(v)
barplot(v)
barplot(head(v))
view(wordsStem)
View(wordsStem)
ngrams <- TermDocumentMatrix(full, control = list(tokenizer = NGramTokenizer))
library(testthat)
#library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
#library(corpus)
library(RWeka)
library(tm)
set.seed(43)
ngrams <- TermDocumentMatrix(full, control = list(tokenizer = NGramTokenizer))
ngrams <- TermDocumentMatrix(full, control = list(tokenizer = NGramTokenizer))
library(NLP)
ngrams <- TermDocumentMatrix(full, control = list(tokenizer = NGramTokenizer))
install.packages("openNLP")
library(openNLP)
ngrams <- TermDocumentMatrix(full, control = list(tokenizer = NGramTokenizer))
ngrams <- TermDocumentMatrix(full, control = list(tokenizer = ngrams))
View(ngrams)
dtmNgram <- as.matrix(ngrams)
dtmNgram
View(dtmNgram)
ngrams <- TermDocumentMatrix(full, control = list(tokenizer = ngrams(min = 2, max =3)))
ngrams <- TermDocumentMatrix(full, control = list(tokenizer = ngrams, min = 2, max = 3))
dtmNgram <- as.matrix(ngrams)
View(dtmNgram)
library("clue")
library("tm")
txt <- system.file("texts", "txt", package = "tm")
(ovid <- Corpus(DirSource(txt),
readerControl = list(reader = readPlain,
language = "la",
load = TRUE)))
ID(ovid[[1]])
(ovid <- Corpus(DirSource(txt),
readerControl = list(reader = readPlain,
language = "la",
load = TRUE)))
ID(ovid[[1]])
ovid[[1]]
Author(ovid[[1]]) <- "Publius Ovidius Naso"
ovid[[1]]
meta(ovid[[1]])
ovid[1:3]
ovid[[1]]
c(ovid[1:2], ovid[3:4])
length(ovid)
summary(ovid)
tmUpdate(ovid, DirSource(txt))
tm_Update(ovid, DirSource(txt))
ovid <- appendMeta(ovid,
cmeta = list(test = c(1,2,3)),
dmeta = list(clust = c(1,1,2,2,2)))
summary(ovid)
CMetaData(ovid)
DMetaData(ovid)
ovid[[1]]$content
library(RWeka)
#library(testthat)
#library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
library(NLP)
library(openNLP)
#library(RWeka)
library(tm)
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), skipNul = TRUE)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), skipNul = TRUE)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), skipNul = TRUE)
profanityList <- lexicon::profanity_banned # Banned words
```{r warning=FALSE, message=FALSE}
#library(testthat)
#library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
library(NLP)
library(openNLP)
#library(RWeka)
library(tm)
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), skipNul = TRUE)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), skipNul = TRUE)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), skipNul = TRUE)
profanityList <- lexicon::profanity_banned # Banned words
docsOrg <- Corpus(VectorSource(c(blogs, news, twitter)), readerControl = list(readPlain,language="en_US", load = TRUE))
rm('./data/Coursera_SwiftKey.zip:final/en_US/en_US.twitter.txt')
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), skipNul = TRUE)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), skipNul = TRUE)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), skipNul = TRUE)
profanityList <- lexicon::profanity_banned # Banned words
rm('./data/Coursera_SwiftKey.zip:final/en_US/en_US.twitter.txt')
rm('./data/Coursera_SwiftKey.zip:final/en_US/en_US.blogs.txt')
rm('./data/Coursera_SwiftKey.zip:final/en_US/en_US.news.txt')
docsOrg <- Corpus(VectorSource(c(blogs, news, twitter)), readerControl = list(readPlain,language="en_US", load = TRUE))
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), skipNul = TRUE)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), skipNul = TRUE)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), skipNul = TRUE)
profanityList <- lexicon::profanity_banned # Banned words
close('./data/Coursera_SwiftKey.zip:final/en_US/en_US.twitter.txt')
close.connection('./data/Coursera_SwiftKey.zip:final/en_US/en_US.twitter.txt')
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), skipNul = TRUE)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), skipNul = TRUE)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), skipNul = TRUE)
profanityList <- lexicon::profanity_banned # Banned words
close.connection('./data/Coursera_SwiftKey.zip:final/en_US/en_US.twitter.txt')
blogsFile <- unz(destiny, 'final/en_US/en_US.blogs.txt')
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogsFile <- unz(destiny, 'final/en_US/en_US.blogs.txt')
blogs <- readLines(blogsFile, skipNul = TRUE)
close(blogsFile)
newsFile <- unz(destiny, 'final/en_US/en_US.news.txt')
news <- readLines(newsFile, skipNul = TRUE)
close(newsFile)
twitterFile <- unz(destiny, 'final/en_US/en_US.twitter.txt')
twitter <- readLines(twitterFile, skipNul = TRUE)
close(twitterFile)
profanityList <- lexicon::profanity_banned # Banned words
docsOrg <- Corpus(VectorSource(c(blogs, news, twitter)), readerControl = list(readPlain,language="en_US", load = TRUE))
rowSums(as.matrix(dtmSamples))
dtmSamples <- DocumentTermMatrix(docsSamples)
tdmOrg <- TermDocumentMatrix(docsOrg)
docsSamples <- Corpus(DirSource('./data/sample'), readerControl = list(readPlain,language="en_US", load = TRUE))
blogsSample <- blogs[rbinom(length(blogs)*.01, length(blogs), 0.5)]
newsSample <- news[rbinom(length(news)*.01, length(news), 0.5)]
twitterSample <- twitter[rbinom(length(twitter)*.01, length(twitter), 0.5)]
rm(blogs) # Remove from memory to save the memory.
rm(news)
rm(twitter)
write.table(blogsSample, file = './data/sample/en_US.blogsSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(newsSample, file = './data/sample/en_US.newsSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(twitterSample, file = './data/sample/en_US.twitterSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(profanityList, file = './data/profanityList.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
docsSamples <- Corpus(DirSource('./data/sample'), readerControl = list(readPlain,language="en_US", load = TRUE))
removeURL <- function(x) gsub("http:[[:alnum:]]*", "", x)
removeHashTags <- function(x) gsub("#\\S+", "", x)
removeTwitterHandles <- function(x) gsub("@\\S+", "", x)
removeHTMLTags <- function(x) gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",x)
docsSamples <- tm_map(docsSamples, content_transformer(tolower))
docsSamples <- tm_map(docsSamples, removeWords, stopwords("english"))
docsSamples <- tm_map(docsSamples, removePunctuation)
docsSamples <- tm_map(docsSamples, removeNumbers)
docsSamples <- tm_map(docsSamples, stripWhitespace)
docsSamples <- tm_map(docsSamples, content_transformer(removeURL))
docsSamples <- tm_map(docsSamples, content_transformer(removeHashTags))
docsSamples <- tm_map(docsSamples, content_transformer(removeTwitterHandles))
docsSamples <- tm_map(docsSamples, content_transformer(removeHTMLTags))
dtmSamples <- DocumentTermMatrix(docsSamples)
rowSums(as.matrix(dtmSamples))
tdmSamples <- TermDocumentMatrix(docsSamples)
tdmMatrix <- as.matrix(tdmSamples)
tdmSamples
View(tdmSamples)
tdmMatrix
hist(tdmMatrix)
barplot(tdmMatrix)
barplot(head(tdmMatrix))
rowSums(as.matrix(dtmSamples))
rowSums(tdmMatrix)
rowSums(tdmMatrix)
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
tdmdf <- data.frame(word = names(tdmMatrixSorted), freq = tdmMatrix)
View(tdmdf)
tdmMatrix <- as.matrix(tdmSamples)
rowSums(tdmMatrix)
tdmMatrix
tdmMatrixSorted
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
tdmdf <- data.frame(word = names(tdmMatrixSorted), freq = tdmMatrix)
View(tdmdf)
View(tdmMatrix)
View(tdmMatrix)
View(tdmOrg)
View(tdmMatrix)
View(tdmOrg)
rowSums(as.matrix(dtmSamples))
tdmMatrix <- as.matrix(tdmSamples)
tdmMatrix
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
tdmMatrixSorted
tdmdf <- data.frame(word = names(tdmMatrixSorted), freq = tdmMatrix)
View(tdmdf)
names(tdmMatrixSorted)
tdmMatrixSorted[1]
tdmMatrixSorted[2]
tdmdf <- data.frame(word = names(tdmMatrixSorted), freq = tdmMatrix)
tdmdf
rowSums(as.matrix(dtmSamples))
tdmMatrix <- as.matrix(tdmSamples)
tdmdf <- data.frame(word = names(tdmMatrixSorted), freq = tdmMatrix)
removeURL <- function(x) gsub("http:[[:alnum:]]*", "", x)
removeHashTags <- function(x) gsub("#\\S+", "", x)
removeTwitterHandles <- function(x) gsub("@\\S+", "", x)
removeHTMLTags <- function(x) gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",x)
docsSamples <- tm_map(docsSamples, content_transformer(tolower))
docsSamples <- tm_map(docsSamples, removeWords, stopwords("english"))
docsSamples <- tm_map(docsSamples, removePunctuation)
docsSamples <- tm_map(docsSamples, removeNumbers)
docsSamples <- tm_map(docsSamples, stripWhitespace)
docsSamples <- tm_map(docsSamples, content_transformer(removeURL))
docsSamples <- tm_map(docsSamples, content_transformer(removeHashTags))
docsSamples <- tm_map(docsSamples, content_transformer(removeTwitterHandles))
docsSamples <- tm_map(docsSamples, content_transformer(removeHTMLTags))
dtmSamples <- DocumentTermMatrix(docsSamples)
rowSums(as.matrix(dtmSamples))
tdmSamples <- TermDocumentMatrix(docsSamples)
tdmMatrix <- as.matrix(tdmSamples)
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
tdmdf <- data.frame(word = names(tdmMatrixSorted), freq = tdmMatrix)
View(tdmMatrix)
tdmdf <- data.frame(word = names(tdmMatrixSorted), freq = tdmMatrixSorted)
View(tdmdf)
barplot(tdmMatrixSorted)
barplot(head(tdmMatrixSorted,10))
data.frame(tdmSamples)
data(tdmSamples)
hclust(dist(tdmSamples), method = "ward")
tdmHClust <- hclust(dist(tdmSamples), method = "ward")
plot(tdmHClust)
rowSums(as.matrix(dtmSamples))
#library(testthat)
#library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
library(NLP)
library(openNLP)
#library(RWeka)
library(tm)
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogsFile <- unz(destiny, 'final/en_US/en_US.blogs.txt')
blogs <- readLines(blogsFile, skipNul = TRUE)
close(blogsFile)
newsFile <- unz(destiny, 'final/en_US/en_US.news.txt')
news <- readLines(newsFile, skipNul = TRUE)
close(newsFile)
twitterFile <- unz(destiny, 'final/en_US/en_US.twitter.txt')
twitter <- readLines(twitterFile, skipNul = TRUE)
close(twitterFile)
profanityList <- lexicon::profanity_banned # Banned words
docsOrg <- Corpus(VectorSource(c(blogs, news, twitter)), readerControl = list(readPlain,language="en_US", load = TRUE))
blogsSample <- blogs[rbinom(length(blogs)*.01, length(blogs), 0.5)]
newsSample <- news[rbinom(length(news)*.01, length(news), 0.5)]
twitterSample <- twitter[rbinom(length(twitter)*.01, length(twitter), 0.5)]
rm(blogs) # Remove from memory to save the memory.
rm(news)
rm(twitter)
write.table(blogsSample, file = './data/sample/en_US.blogsSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(newsSample, file = './data/sample/en_US.newsSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(twitterSample, file = './data/sample/en_US.twitterSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(profanityList, file = './data/profanityList.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
docsSamples <- Corpus(DirSource('./data/sample'), readerControl = list(readPlain,language="en_US", load = TRUE))
removeURL <- function(x) gsub("http:[[:alnum:]]*", "", x)
removeHashTags <- function(x) gsub("#\\S+", "", x)
removeTwitterHandles <- function(x) gsub("@\\S+", "", x)
removeHTMLTags <- function(x) gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",x)
docsSamples <- tm_map(docsSamples, content_transformer(tolower))
docsSamples <- tm_map(docsSamples, removeWords, stopwords("english"))
docsSamples <- tm_map(docsSamples, removePunctuation)
docsSamples <- tm_map(docsSamples, removeNumbers)
docsSamples <- tm_map(docsSamples, stripWhitespace)
docsSamples <- tm_map(docsSamples, content_transformer(removeURL))
docsSamples <- tm_map(docsSamples, content_transformer(removeHashTags))
docsSamples <- tm_map(docsSamples, content_transformer(removeTwitterHandles))
docsSamples <- tm_map(docsSamples, content_transformer(removeHTMLTags))
dtmSamples <- DocumentTermMatrix(docsSamples)
rowSums(as.matrix(dtmSamples))
tdmSamples <- TermDocumentMatrix(docsSamples)
tdmMatrix <- as.matrix(tdmSamples)
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
library("clue")
library("tm")
install.packages("clue")
library("clue")
library("tm")
library(RWeka)
library(Weka)
library(RWeka)
