<<<<<<< Updated upstream
meta(docsOrg[1])
meta(docsOrg[2])
meta(docsOrg[[2]])
meta(docsOrg[[3]])
meta(docsOrg[[2]],"id")
inspect(docsOrg[1:2])
lapply(docsOrg[1:2], as.character)
inspect(docsOrg[[1]])
meta(docsOrg[[1]])
meta(dtmOrg)
inspect(dtmOrg)
dtmOrg$dimnames
dtmOrg$i
dtmOrg$j
dtmOrg$nrow
dtmOrg$ncol
dtmOrg$dimnames
dtmOrg[blogs]
dtmOrg['blogs']
dtmOrg["blogs"]
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
tdmMatrix <- as.matrix(tdmSamples)
rowSums(as.matrix(dtmSamples))
dtm
dtmOrg
docsOrg
docsOrg[["blogs"]]
meta(docsOrg)
meta(docsOrg[1])
meta(docsOrg$content)
Sys.setlocale("LC_ALL", "American")
#library(testthat)
#library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
library(NLP)
library(openNLP)
library(RWeka)
library(tm)
=======
rm('./data/Coursera_SwiftKey.zip:final/en_US/en_US.twitter.txt')
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), skipNul = TRUE)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), skipNul = TRUE)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), skipNul = TRUE)
profanityList <- lexicon::profanity_banned # Banned words
rm('./data/Coursera_SwiftKey.zip:final/en_US/en_US.twitter.txt')
rm('./data/Coursera_SwiftKey.zip:final/en_US/en_US.blogs.txt')
rm('./data/Coursera_SwiftKey.zip:final/en_US/en_US.news.txt')
docsOrg <- Corpus(VectorSource(c(blogs, news, twitter)), readerControl = list(readPlain,language="en_US", load = TRUE))
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), skipNul = TRUE)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), skipNul = TRUE)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), skipNul = TRUE)
profanityList <- lexicon::profanity_banned # Banned words
close('./data/Coursera_SwiftKey.zip:final/en_US/en_US.twitter.txt')
close.connection('./data/Coursera_SwiftKey.zip:final/en_US/en_US.twitter.txt')
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), skipNul = TRUE)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), skipNul = TRUE)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), skipNul = TRUE)
profanityList <- lexicon::profanity_banned # Banned words
close.connection('./data/Coursera_SwiftKey.zip:final/en_US/en_US.twitter.txt')
blogsFile <- unz(destiny, 'final/en_US/en_US.blogs.txt')
>>>>>>> Stashed changes
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogsFile <- unz(destiny, 'final/en_US/en_US.blogs.txt')
blogs <- readLines(blogsFile, skipNul = TRUE)
close(blogsFile)
newsFile <- unz(destiny, 'final/en_US/en_US.news.txt')
news <- readLines(newsFile, skipNul = TRUE)
close(newsFile)
twitterFile <- unz(destiny, 'final/en_US/en_US.twitter.txt')
twitter <- readLines(twitterFile, skipNul = TRUE)
close(twitterFile)
profanityList <- lexicon::profanity_banned # Banned words
#docsOrg <- Corpus(DirSource(unz(destiny, 'final/en_US/')), readerControl = list(readPlain,language="en_US", load = TRUE))
docsOrg <- Corpus(VectorSource(c(blogs, news, twitter)), readerControl = list(readPlain,language="en_US", load = TRUE))
blogsSample <- blogs[rbinom(length(blogs)*.01, length(blogs), 0.5)]
newsSample <- news[rbinom(length(news)*.01, length(news), 0.5)]
twitterSample <- twitter[rbinom(length(twitter)*.01, length(twitter), 0.5)]
write.table(blogsSample, file = './data/sample/en_US.blogsSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(newsSample, file = './data/sample/en_US.newsSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(twitterSample, file = './data/sample/en_US.twitterSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(profanityList, file = './data/profanityList.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
docsSamples <- Corpus(DirSource('./data/sample'), readerControl = list(readPlain,language="en_US", load = TRUE))
docsOrg <- Corpus(DirSource('./data/final/en_US'), readerControl = list(readPlain,language="en_US", load = TRUE))
docsOrg$en_US.blogs.txt
docsOrg$en_US.news.txt
meta(docsOrg$en_US.blogs.txt)
inspect(docsOrg$en_US.blogs.txt)
docsOrg <- VCorpus(DirSource('./data/final/en_US'), readerControl = list(readPlain,language="en_US", load = TRUE))
docsOrg$en_US.blogs.txt
inspect(docsOrg$en_US.blogs.txt)
meta(docsOrg)
meta(docsOrg$en_US.blogs.txt)
meta(docsOrg$en_US.blogs.txt$content)
docsOrg <- Corpus(VectorSource(c(blogs, news, twitter)), readerControl = list(readPlain,language="en_US", load = TRUE))
meta(docsOrg$en_US.blogs.txt)
blogsOrg <- Corpus(VectorSource(blogs), readerControl = list(readPlain,language="en_US", load = TRUE))
newsOrg <- Corpus(VectorSource(news), readerControl = list(readPlain,language="en_US", load = TRUE))
twitterOrg <- Corpus(VectorSource(twitter), readerControl = list(readPlain,language="en_US", load = TRUE))
meta(blogsOrg)
inspect(blogsOrg)
inspect(blogsOrg[[1]])
txt <- system.file("texts", "txt", package = "tm")
(ovid <- VCorpus(DirSource(txt, encoding = "UTF-8"),
readerControl = list(language = "lat")))
docs <- c("This is a text", "This another one") # from a character vector
VCorpus(VectorSource(docs))
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- VCorpus(DirSource(reut21578, mode = "binary"),
readerControl = list(reader = readReut21578XMLasPlain))
inspect(dtm)
dtmBlogs <- DocumentTermMatrix(blogsOrg)
dtmNews <- DocumentTermMatrix(newsOrg)
dtmTwitter <- DocumentTermMatrix(twitterOrg)
inspect(blogsOrg[[1]])
inspect(blogsOrg)
head(inspect(blogsOrg))
summary(docsOrg)
docsOrg <- Corpus(DirSource('./data/final/en_US'), readerControl = list(readPlain,language="en_US", load = TRUE))
summary(docsOrg)
summary(docsOrg$en_US.blogs.txt)
docsOrg <- Corpus(VectorSource(c(blogs, news, twitter)), readerControl = list(readPlain,language="en_US", load = TRUE))
summary(blogsOrg)
dtmOrg
dtmBlogs
View(dtmBlogs)
dtmSamples
rowSums(as.matrix(dtmSamples))
source(word_count)
source('C:/Users/hongji/GitHub/Capstone/word_count.R', echo=TRUE)
word_count(newsFile)
word_count('./data/sample/en_US.twitterSample.csv')
word_count('./data/sample/en_US.blogsSample.csv')
f.word.count <- function(my.list) { sum(stringr::str_count(my.list, "\\S+")) }
df <- data.frame(text.source = c("blogs", "twitter", "news"), line.count = NA, word.count = NA)
my.list <- list(blogs = blogs, twitter = twitter, news = news)
df$line.count <- sapply(my.list, length)
df$line.count
df$word.count <- sapply(my.list, f.word.count)
df
g.line.count <- ggplot(df, aes(x = factor(text.source), y = line.count/1e+06))
library(ggplot2)
library(ggplot2)
g.line.count <- ggplot(df, aes(x = factor(text.source), y = line.count/1e+06))
g.line.count <- g.line.count + geom_bar(stat = "identity") +
labs(y = "# of lines/million", x = "text source", title = "Count of lines per Corpus")
g.word.count <- ggplot(df, aes(x = factor(text.source), y = word.count/1e+06))
g.word.count <- g.word.count + geom_bar(stat = "identity") +
labs(y = "# of words/million", x = "text source", title = "Count of words per Corpus")
g.line.count
g.word.count
length(blogs)
# Data frame to store counts
dfSample <- data.frame(text.source = c("blogs_Sample", "twitter_Sample", "news_Sample"), line.count = NA, word.count = NA)
# store the files into a list
my.list <- list(blogs_Sample = blogsSample, twitter_sample = twitterSample, news_Sample = twitterSample)
# Counts lines and words
dfSample$line.count <- sapply(my.list, length)
dfSample$word.count <- sapply(my.list, f.word.count)
g.line.count <- ggplot(dfOrg, aes(x = factor(text.source), y = line.count/1e+06))
# Data frame to store counts
dfSample <- data.frame(text.source = c("blogs_Sample", "twitter_Sample", "news_Sample"), line.count = NA, word.count = NA)
# store the files into a list
my.list <- list(blogs_Sample = blogsSample, twitter_sample = twitterSample, news_Sample = twitterSample)
# Counts lines and words
dfSample$line.count <- sapply(my.list, length)
dfSample$word.count <- sapply(my.list, f.word.count)
g.line.count <- ggplot(dfSample, aes(x = factor(text.source), y = line.count/1e+06))
g.line.count <- g.line.count + geom_bar(stat = "identity") +
labs(y = "# of lines/million", x = "text source", title = "Count of lines per Sample Corpus")
g.word.count <- ggplot(dfSample, aes(x = factor(text.source), y = word.count/1e+06))
g.word.count <- g.word.count + geom_bar(stat = "identity") +
labs(y = "# of words/million", x = "text source", title = "Count of words per Sample Corpus")
# Data frame to store counts
dfSample <- data.frame(text.source = c("blogs_Sample", "twitter_Sample", "news_Sample"), line.count = NA, word.count = NA)
# store the files into a list
my.list <- list(blogs_Sample = blogsSample, twitter_sample = twitterSample, news_Sample = twitterSample)
# Counts lines and words
dfSample$line.count <- sapply(my.list, length)
dfSample$word.count <- sapply(my.list, f.word.count)
g.line.count <- ggplot(dfSample, aes(x = factor(text.source), y = line.count/1e+06))
g.line.count <- g.line.count + geom_bar(stat = "identity") +
labs(y = "# of lines/million", x = "text source", title = "Count of lines per Sample Corpus")
g.word.count <- ggplot(dfSample, aes(x = factor(text.source), y = word.count/1e+06))
g.word.count <- g.word.count + geom_bar(stat = "identity") +
labs(y = "# of words/million", x = "text source", title = "Count of words per Sample Corpus")
g.line.count
g.line.count
# Data frame to store counts
dfSample <- data.frame(text.source = c("blogs_Sample", "twitter_Sample", "news_Sample"), line.count = NA, word.count = NA)
# store the files into a list
my.list <- list(blogs_Sample = blogsSample, twitter_sample = twitterSample, news_Sample = twitterSample)
# Counts lines and words
dfSample$line.count <- sapply(my.list, length)
dfSample$word.count <- sapply(my.list, f.word.count)
g.line.count <- ggplot(dfSample, aes(x = factor(text.source), y = line.count/1e+03))
g.line.count <- g.line.count + geom_bar(stat = "identity") +
labs(y = "# of lines/million", x = "text source", title = "Count of lines per Sample Corpus")
g.word.count <- ggplot(dfSample, aes(x = factor(text.source), y = word.count/1e+03))
g.word.count <- g.word.count + geom_bar(stat = "identity") +
labs(y = "# of words/million", x = "text source", title = "Count of words per Sample Corpus")
g.line.count
g.line.count
# Data frame to store counts
dfSample <- data.frame(text.source = c("blogs_Sample", "twitter_Sample", "news_Sample"), line.count = NA, word.count = NA)
# store the files into a list
my.list <- list(blogs_Sample = blogsSample, twitter_sample = twitterSample, news_Sample = twitterSample)
# Counts lines and words
dfSample$line.count <- sapply(my.list, length)
dfSample$word.count <- sapply(my.list, f.word.count)
g.line.count <- ggplot(dfSample, aes(x = factor(text.source), y = line.count/1e+03))
g.line.count <- g.line.count + geom_bar(stat = "identity") +
labs(y = "# of lines/thousands", x = "text source", title = "Count of lines per Sample Corpus")
g.word.count <- ggplot(dfSample, aes(x = factor(text.source), y = word.count/1e+03))
g.word.count <- g.word.count + geom_bar(stat = "identity") +
labs(y = "# of words/thousands", x = "text source", title = "Count of words per Sample Corpus")
g.line.count
g.word.count
dfSample
g.line.count <- ggplot(dfSample, aes(x = factor(text.source), y = line.count/1e+03))
# Data frame to store counts
dfSample <- data.frame(text.source = c("blogs_Sample", "twitter_Sample", "news_Sample"), line.count = NA, word.count = NA)
# store the files into a list
my.list <- list(blogs_Sample = blogsSample, twitter_sample = twitterSample, news_Sample = newsSample)
# Counts lines and words
dfSample$line.count <- sapply(my.list, length)
dfSample$word.count <- sapply(my.list, f.word.count)
g.line.count <- ggplot(dfSample, aes(x = factor(text.source), y = line.count/1e+03))
g.line.count <- g.line.count + geom_bar(stat = "identity") +
labs(y = "# of lines/thousands", x = "text source", title = "Count of lines per Sample Corpus")
g.word.count <- ggplot(dfSample, aes(x = factor(text.source), y = word.count/1e+03))
g.word.count <- g.word.count + geom_bar(stat = "identity") +
labs(y = "# of words/thousands", x = "text source", title = "Count of words per Sample Corpus")
g.line.count
g.word.count
removeURL <- function(x) gsub("http:[[:alnum:]]*", "", x)
removeHashTags <- function(x) gsub("#\\S+", "", x)
removeTwitterHandles <- function(x) gsub("@\\S+", "", x)
removeHTMLTags <- function(x) gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",x)
docsSamples <- tm_map(docsSamples, content_transformer(tolower))
docsSamples <- tm_map(docsSamples, removeWords, stopwords("english"))
docsSamples <- tm_map(docsSamples, removePunctuation)
docsSamples <- tm_map(docsSamples, removeNumbers)
docsSamples <- tm_map(docsSamples, stripWhitespace)
docsSamples <- tm_map(docsSamples, content_transformer(removeURL))
docsSamples <- tm_map(docsSamples, content_transformer(removeHashTags))
docsSamples <- tm_map(docsSamples, content_transformer(removeTwitterHandles))
docsSamples <- tm_map(docsSamples, content_transformer(removeHTMLTags))
dtmSamples <- DocumentTermMatrix(docsSamples)
rowSums(as.matrix(dtmSamples))
tdmSamples <- TermDocumentMatrix(docsSamples)
tdmMatrix <- as.matrix(tdmSamples)
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
#tdmSamples <- TermDocumentMatrix(docsSamples, control = list(tokenize = NLP::wordpunct_tokenizer))
#tdmTwitter <- TermDocumentMatrix(docsSamples["en_US.twitterSample.csv"], control = list(wordLengths = c(3,Inf)))
#tdmBlog <- TermDocumentMatrix(docsSamples["en_US.blogsSample.csv"], control = list(wordLengths = c(3,Inf)))
#tdmNews <- TermDocumentMatrix(docsSamples["en_US.newsSample.csv"], control = list(wordLengths = c(3,Inf)))
#tdmDocsSamples <- TermDocumentMatrix(docsSamples, control = list(wordLengths = c(3,Inf)))
tdmMatrix
tdmMatrixSorted
length(dtmSamples)
length(dtmSamples[[1]])
length(dtmSamples[[2]])
length(dtmSamples[[3]])
length(dtmSamples$nrow)
length(dtmSamples$ncol)
length(dtmSamples$dimnames)
length(dtmSamples$i)
length(dtmSamples$j)
length(dtmSamples$k)
length(dtmSamples$v)
as.matrix(tdmSamples)
rowSums(tdmMatrix)
sort(rowSums(tdmMatrix), decreasing = TRUE)
rowSums(as.matrix(dtmSamples))
names(as.matrix(dtmSamples))
as.matrix(dtmSamples)[1,1]
as.matrix(dtmSamples)[1,2]
as.matrix(dtmSamples)[1,3]
as.matrix(dtmSamples)[2,1]
as.matrix(dtmSamples)[2,2]
length(tdmMatrix)
length(tdmSamples)
length(tdmSamples[[]])
length(tdmSamples[[1]])
tdmSamples
data.frame(tdmSamples)
dtmSamples
tdmSamples
barplot(rowSums(as.matrix(dtmSamples)))
barplot(rowSums(as.matrix(dtmSamples)))
rowSums(as.matrix(dtmSamples))
barplot(rowSums(as.matrix(tdmMatrixSorted)))
barplot(rowSums(as.matrix(tdmMatrixSorted)))
barplot(head(rowSums(as.matrix(tdmMatrixSorted))))
barplot(head(rowSums(as.matrix(tdmMatrixSorted))))
wordFreq <- data.frame(words = tdmSamples$dimnames$Terms, frequency = tdmSamples$v)
wordFreq <- data.frame(words = tdmSamples$dimnames$Terms, frequency = tdmSamples$i)
wordFreq <- data.frame(words = tdmSamples$dimnames$Terms, frequency = tdmSamples$v)
wordFreq <- data.frame(words = tdmSamples$dimnames$Terms, frequency = tdmSamples$nrow)
wordFreq
wordFreq <- data.frame(words = dtmSamples$dimnames$Terms, frequency = dtmSamples$i)
wordFreq <- data.frame(words = dtmSamples$dimnames$Terms, frequency = dtmSamples$j)
wordFreq <- data.frame(words = dtmSamples$dimnames$Terms, frequency = dtmSamples$i)
wordFreq <- data.frame(words = dtmSamples$dimnames$Terms, frequency = dtmSamples$i)
wordFreqBlog <- data.frame(words = dtmBlogs$dimnames$Terms, frequency = dtmBlogs$v)
findFreqTerms(tdmSamples)
findFreqTerms(tdmSamples,5)
findFreqTerms(tdmSamples,100)
findFreqTerms(tdmSamples,1000)
library(stopwords)
library(sentimentr)
library(dplyr)
library(ggplot2)
library(NLP)
library(openNLP)
library(RWeka)
library(tm)
tdmSamples <- TermDocumentMatrix(docsSamples, control = list(tokenizer = RWeka::WordTokenizer()))
tdmSamples <- TermDocumentMatrix(docsSamples, control = list(tokenize = RWeka::WordTokenizer()))
tdmSamples <- TermDocumentMatrix(docsSamples, control = list(tokenize = RWeka::WordTokenizer))
wordFreqBlog <- data.frame(words = dtmBlogs$dimnames$Terms, frequency = dtmBlogs$v)
wordFreq <- data.frame(words = tdmSamples$dimnames$Terms, frequency = tdmSamples$v)
tdmSamples <- TermDocumentMatrix(docsSamples, control = list(tokenize = RWeka::WordTokenizer))
inspect(tdmSamples)
inspect(tdmSamples$dimnames$Docs)
inspect(tdmSamples[[en_US.blogsSample.csv]])
inspect(tdmSamples[['en_US.blogsSample.csv']])
inspect(tdmSamples$dimnames[['en_US.blogsSample.csv']])
inspect(tdmSamples$[['en_US.blogsSample.csv']])
inspect(tdmSamples['en_US.blogsSample.csv'])
inspect(tdmSamples[['en_US.blogsSample.csv']])
inspect(tdmSamples$dimnames[["en_US.blogsSample.csv"]])
inspect(dtm)
inspect(tdm)
dtm <- DocumentTermMatrix(reuters)
tdm <- DocumentTermMatrix(reuters)
inspect(dtm)
inspect(tdm)
tdm <- TermDocumentMatrix(reuters)
inspect(dtm)
inspect(tdm)
meta(tdmSamples)
meta(tdmSamples, "headings")
meta(tdmSamples, "heading")
inspect(tdmSamples[[1]])
inspect(reuters[[1]])
inspect(reuters[[1]][1])
inspect(reuters[[1]][2])
inspect(reuters[[1]][3])
inspect(reuters[[2]])
inspect(tdmSamples[[1]][1])
inspect(tdmSamples$dimnames$Docs)
tdmSamples$dimnames$Docs
View(tdmSamples)
tdmSamples[["dimnames"]][["Docs" == 'en_US.blogsSample.csv']]
tdmSamples[["dimnames"]][["Docs" == "en_US.blogsSample.csv"]]
tdmMatrix
View(tdmMatrix)
tdmSamples <- TermDocumentMatrix(docsSamples, control = list(tokenize = RWeka::WordTokenizer))
tdmMatrix <- as.matrix(tdmSamples)
View(tdmMatrix)
tdmMatrixSorted
tdmMatrixSorted[1]
tdmMatrixSorted[1,2]
tdmMatrixSorted[1:2]
names(tdmMatrixSorted)
colSums(tdmMatrix)
rowSums(tdmMatrix)
tdmSamples
as.matrix(tdmSamples)
tdmMatrix
tdmMatrix[1]
tdmMatrix[2]
tdmMatrix[1,2]
tdmMatrix[1:2]
names(tdmMatrix)
tdmMatrix[[1]]
tdmMatrix[[1]][[2]]
tdmMatrix[[1]][2]
matrix()
dim(tdmMatrix)
tdmMatrix[,3]
colSums(tdmMatrix)
tdmSamples
sum(tdmMatrix)
count(findFreqTerms(tdmSamples, 2))
findFreqTerms(tdmSamples,2)
?inspect
inspect(tdm)
inspect(dtm)
length(tdmMatrix)
length(tdmMatrix)/3
length(tdmMatrix[,1])
freqWords <- findFreqTerms(tdmMatrix, 2)
findFreqTerms(tdmMatrix, 2)
freqWords <- findFreqTerms(tdmSamples, 2)
freqWords
length(freqWords)
tdmMatrixSorted[1:n,]
n <- 25L # variable to set top n words
tdmMatrixSorted[1:n,]
tdmMatrixSorted[1:n]
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top <- ggplot(tdmMatrixSorted[1:n])
names(tdmMatrixSorted)
?data.frame
tdmMatrixSorted <- data.frame(row.names = names(tdmMatrixSorted), frequency = tdmMatrixSorted)
tdmMatrixSorted
g.top <- ggplot(tdmMatrixSorted[1:n])
tdmMatrixSorted["frequency"]
tdmMatrixSorted[]
tdmMatrixSorted[frequency]
tdmMatrixSorted["frequency"]
g.top <- ggplot(tdmMatrixSorted, aes(x = word, y = frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
g.top <- ggplot(tdmMatrixSorted, aes(x = words, y = frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
g.top <- ggplot(tdmMatrixSorted, aes(x = words, y = tdmMatrixSorted$frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
g.top <- ggplot(tdmMatrixSorted, aes(x = tdmMatrixSorted$words, y = tdmMatrixSorted$frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
View(tdmMatrixSorted)
tdmMatrixSorted <- data.frame(words = names(tdmMatrixSorted), frequency = tdmMatrixSorted)
g.top <- ggplot(tdmMatrixSorted, aes(x = tdmMatrixSorted$words, y = tdmMatrixSorted$frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
tdmMatrixSorted
tdmMatrixSorted
tdmMatrixSorted <- data.frame(words = names(tdmMatrixSorted), frequency = tdmMatrixSorted)
rm(tdmMatrixSorted)
tdmMatrixSorted <- data.frame(words = names(tdmMatrixSorted), frequency = tdmMatrixSorted)
g.top <- ggplot(tdmMatrixSorted, aes(x = tdmMatrixSorted$words, y = tdmMatrixSorted$frequency))
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
tdmMatrixSorted <- data.frame(words = names(tdmMatrixSorted), frequency = tdmMatrixSorted)
View(tdmMatrixSorted)
g.top <- ggplot(tdmMatrixSorted, aes(x = tdmMatrixSorted$words, y = tdmMatrixSorted$frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
g.top <- ggplot(tdmMatrixSorted[1:n,], aes(x = tdmMatrixSorted$words, y = tdmMatrixSorted$frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
dfTop <- data.frame(words = names(tdmMatrixSorted), frequency = tdmMatrixSorted)
g.top <- ggplot(dfTop[1:n,], aes(x = dfTop$words, y = dfTop$frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
dfTop <- data.frame(words = names(tdmMatrixSorted[1:n]), frequency = tdmMatrixSorted[1:n])
g.top <- ggplot(dfTop, aes(x = dfTop$words, y = dfTop$frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
dfTop <- data.frame(words = names(tdmMatrixSorted[1:n]), frequency = tdmMatrixSorted[1:n])
g.top <- ggplot(dfTop, aes(x = dfTop$words, y = dfTop$frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
dfTop$words <- reorder(dfTop$words, dfTop$frequency)
g.top <- ggplot(dfTop, aes(x = dfTop$words, y = dfTop$frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
g.top <- ggplot(dfTop, aes(x = words, y = frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
dtmSamplesBigram <- DocumentTermMatrix(dtmSamples, control = list(tokenize = BigramTokenizer))
library(stopwords)
library(sentimentr)
library(dplyr)
library(ggplot2)
library(NLP)
library(openNLP)
library(RWeka)
library(tm)
#library(testthat)
#library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
library(ggplot2)
library(NLP)
library(openNLP)
library(RWeka)
library(tm)
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogsFile <- unz(destiny, 'final/en_US/en_US.blogs.txt')
blogs <- readLines(blogsFile, skipNul = TRUE)
close(blogsFile)
newsFile <- unz(destiny, 'final/en_US/en_US.news.txt')
news <- readLines(newsFile, skipNul = TRUE)
close(newsFile)
twitterFile <- unz(destiny, 'final/en_US/en_US.twitter.txt')
twitter <- readLines(twitterFile, skipNul = TRUE)
close(twitterFile)
profanityList <- lexicon::profanity_banned # Banned words
#docsOrg <- Corpus(DirSource(unz(destiny, 'final/en_US/')), readerControl = list(readPlain,language="en_US", load = TRUE))
#blogsOrg <- Corpus(VectorSource(blogs), readerControl = list(readPlain,language="en_US", load = TRUE))
#newsOrg <- Corpus(VectorSource(news), readerControl = list(readPlain,language="en_US", load = TRUE))
#twitterOrg <- Corpus(VectorSource(twitter), readerControl = list(readPlain,language="en_US", load = TRUE))
docsOrg <- Corpus(VectorSource(c(blogs, news, twitter)), readerControl = list(readPlain,language="en_US", load = TRUE))
#docsOrg <- Corpus(DirSource('./data/final/en_US'), readerControl = list(readPlain,language="en_US", load = TRUE))
#tdmOrg <- TermDocumentMatrix(docsOrg)
#dtmBlogs <- DocumentTermMatrix(blogsOrg)
#dtmNews <- DocumentTermMatrix(newsOrg)
#dtmTwitter <- DocumentTermMatrix(twitterOrg)
dtmOrg <- DocumentTermMatrix(docsOrg)
#length(docsOrg)
#object.size(dtmOrg)
blogsSample <- blogs[rbinom(length(blogs)*.01, length(blogs), 0.5)]
newsSample <- news[rbinom(length(news)*.01, length(news), 0.5)]
twitterSample <- twitter[rbinom(length(twitter)*.01, length(twitter), 0.5)]
rm(blogs) # Remove from memory to save the memory.
rm(news)
rm(twitter)
rm(docsOrg)
rm(dtmOrg)
write.table(blogsSample, file = './data/sample/en_US.blogsSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(newsSample, file = './data/sample/en_US.newsSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(twitterSample, file = './data/sample/en_US.twitterSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(profanityList, file = './data/profanityList.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
<<<<<<< Updated upstream
=======
docsSamples <- Corpus(DirSource('./data/sample'), readerControl = list(readPlain,language="en_US", load = TRUE))
removeURL <- function(x) gsub("http:[[:alnum:]]*", "", x)
removeHashTags <- function(x) gsub("#\\S+", "", x)
removeTwitterHandles <- function(x) gsub("@\\S+", "", x)
removeHTMLTags <- function(x) gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",x)
docsSamples <- tm_map(docsSamples, content_transformer(tolower))
docsSamples <- tm_map(docsSamples, removeWords, stopwords("english"))
docsSamples <- tm_map(docsSamples, removePunctuation)
docsSamples <- tm_map(docsSamples, removeNumbers)
docsSamples <- tm_map(docsSamples, stripWhitespace)
docsSamples <- tm_map(docsSamples, content_transformer(removeURL))
docsSamples <- tm_map(docsSamples, content_transformer(removeHashTags))
docsSamples <- tm_map(docsSamples, content_transformer(removeTwitterHandles))
docsSamples <- tm_map(docsSamples, content_transformer(removeHTMLTags))
dtmSamples <- DocumentTermMatrix(docsSamples)
rowSums(as.matrix(dtmSamples))
tdmSamples <- TermDocumentMatrix(docsSamples)
tdmMatrix <- as.matrix(tdmSamples)
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
library("clue")
library("tm")
install.packages("clue")
library("clue")
library("tm")
library(RWeka)
library(Weka)
library(RWeka)
library(stopwords)
install.packages("stopwords")
install.packages("sentimentr")
install.packages("openNLP")
install.packages("NLP")
install.packages("RWeka")
install.packages("tm")
library(stopwords)
library(sentimentr)
library(dplyr)
library(ggplot2)
library(NLP)
library(openNLP)
install.packages("openNLP")
library(openNLP)
library(RWeka)
library(tm)
library(openNLP)
install.packages("rJava")
library(openNLP)
install.packages("rJava")
library(openNLP)
library(openNLP)
library(RWeka)
library(rJava)
library(RWeka)
library(RWeka)
library(stopwords)
library(sentimentr)
library(dplyr)
library(ggplot2)
library(NLP)
library(openNLP)
library(RWeka)
library(tm)
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists('./data/sample')){
dir.create('./data/sample')
} else {
print("the directory already exists")
}
#library(testthat)
#library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
library(ggplot2)
library(NLP)
library(openNLP)
library(RWeka)
library(tm)
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogsFile <- unz(destiny, 'final/en_US/en_US.blogs.txt')
blogs <- readLines(blogsFile, skipNul = TRUE)
blogs <- readLines(blogsFile, skipNul = TRUE)
blogsFile <- unz(destiny, 'final/en_US/en_US.blogs.txt')
blogs <- readLines(blogsFile, skipNul = TRUE)
close(blogsFile)
newsFile <- unz(destiny, 'final/en_US/en_US.news.txt')
news <- readLines(newsFile, skipNul = TRUE)
close(newsFile)
twitterFile <- unz(destiny, 'final/en_US/en_US.twitter.txt')
twitter <- readLines(twitterFile, skipNul = TRUE)
close(twitterFile)
profanityList <- lexicon::profanity_banned # Banned words
#docsOrg <- Corpus(DirSource(unz(destiny, 'final/en_US/')), readerControl = list(readPlain,language="en_US", load = TRUE))
#blogsOrg <- Corpus(VectorSource(blogs), readerControl = list(readPlain,language="en_US", load = TRUE))
#newsOrg <- Corpus(VectorSource(news), readerControl = list(readPlain,language="en_US", load = TRUE))
#twitterOrg <- Corpus(VectorSource(twitter), readerControl = list(readPlain,language="en_US", load = TRUE))
docsOrg <- Corpus(VectorSource(c(blogs, news, twitter)), readerControl = list(readPlain,language="en_US", load = TRUE))
#docsOrg <- Corpus(DirSource('./data/final/en_US'), readerControl = list(readPlain,language="en_US", load = TRUE))
#tdmOrg <- TermDocumentMatrix(docsOrg)
#dtmBlogs <- DocumentTermMatrix(blogsOrg)
#dtmNews <- DocumentTermMatrix(newsOrg)
#dtmTwitter <- DocumentTermMatrix(twitterOrg)
dtmOrg <- DocumentTermMatrix(docsOrg)
# Helper funciton to count words. \\S+ = every word between space (single or multiple spaces)
f.word.count <- function(my.list) {sum(stringr::str_count(my.list,"\\S+"))}
# Data frame to store counts
dfOrg <- data.frame(text.source = c("blogs", "twitter", "news"), line.count = NA, word.count = NA)
# store the files into a list
my.list <- list(blogs = blogs, twitter = twitter, news = news)
# Counts lines and words
dfOrg$line.count <- sapply(my.list, length)
dfOrg$word.count <- sapply(my.list, f.word.count)
g.line.count <- ggplot(dfOrg, aes(x = factor(text.source), y = line.count/1e+06))
g.line.count <- g.line.count + geom_bar(stat = "identity") +
labs(y = "# of lines/million", x = "text source", title = "Count of lines per Corpus")
g.word.count <- ggplot(dfOrg, aes(x = factor(text.source), y = word.count/1e+06))
g.word.count <- g.word.count + geom_bar(stat = "identity") +
labs(y = "# of words/million", x = "text source", title = "Count of words per Corpus")
blogsSample <- blogs[rbinom(length(blogs)*.01, length(blogs), 0.5)]
newsSample <- news[rbinom(length(news)*.01, length(news), 0.5)]
twitterSample <- twitter[rbinom(length(twitter)*.01, length(twitter), 0.5)]
rm(blogs) # Remove from memory to save the memory.
rm(news)
rm(twitter)
rm(docsOrg)
rm(dtmOrg)
write.table(blogsSample, file = './data/sample/en_US.blogsSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(newsSample, file = './data/sample/en_US.newsSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(twitterSample, file = './data/sample/en_US.twitterSample.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
write.table(profanityList, file = './data/profanityList.csv', row.names = FALSE, col.names = FALSE, sep = ",", quote = FALSE)
docsSamples <- Corpus(VectorSource(c(blogsSample, newsSample, twitterSample)), readerControl = list(readPlain,language="en_US", load = TRUE))
blogsSample <- blogs[rbinom(length(blogs)*.01, length(blogs), 0.5)]
docsSamples <- VCorpus(VectorSource(c(blogsSample, newsSample, twitterSample)), readerControl = list(readPlain,language="en_US", load = TRUE))
# Data frame to store counts
dfSample <- data.frame(text.source = c("blogs_Sample", "twitter_Sample", "news_Sample"), line.count = NA, word.count = NA)
# Data frame to store counts
dfSample <- data.frame(text.source = c("blogs_Sample", "twitter_Sample", "news_Sample"), line.count = NA, word.count = NA)
# store the files into a list
my.list <- list(blogs_Sample = blogsSample, twitter_sample = twitterSample, news_Sample = newsSample)
# Counts lines and words
dfSample$line.count <- sapply(my.list, length)
dfSample$word.count <- sapply(my.list, f.word.count)
g.line.count <- ggplot(dfSample, aes(x = factor(text.source), y = line.count/1e+03))
g.line.count <- g.line.count + geom_bar(stat = "identity") +
labs(y = "# of lines/thousands", x = "text source", title = "Count of lines per Sample Corpus")
g.word.count <- ggplot(dfSample, aes(x = factor(text.source), y = word.count/1e+03))
g.word.count <- g.word.count + geom_bar(stat = "identity") +
labs(y = "# of words/thousands", x = "text source", title = "Count of words per Sample Corpus")
g.line.count
g.word.count
removeURL <- function(x) gsub("http:[[:alnum:]]*", "", x)
removeHashTags <- function(x) gsub("#\\S+", "", x)
removeTwitterHandles <- function(x) gsub("@\\S+", "", x)
removeHTMLTags <- function(x) gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",x)
docsSamples <- tm_map(docsSamples, content_transformer(tolower))
docsSamples <- tm_map(docsSamples, removeWords, stopwords("english"))
docsSamples <- tm_map(docsSamples, removePunctuation)
docsSamples <- tm_map(docsSamples, removeNumbers)
docsSamples <- tm_map(docsSamples, stripWhitespace)
#docsSamples <- tm_map(docsSamples, stemDocument)
docsSamples <- tm_map(docsSamples, content_transformer(removeURL))
docsSamples <- tm_map(docsSamples, content_transformer(removeHashTags))
docsSamples <- tm_map(docsSamples, content_transformer(removeTwitterHandles))
docsSamples <- tm_map(docsSamples, content_transformer(removeHTMLTags))
dtmSamples <- DocumentTermMatrix(docsSamples)
#rowSums(as.matrix(dtmSamples))
tdmSamples <- TermDocumentMatrix(docsSamples, control = list(tokenize = RWeka::WordTokenizer))
tdmMatrix <- as.matrix(tdmSamples)
tdmMatrixSorted <- sort(rowSums(tdmMatrix), decreasing = TRUE)
#tdmSamples <- TermDocumentMatrix(docsSamples, control = list(tokenize = NLP::wordpunct_tokenizer))
sum(tdmMatrix)
length(tdmMatrix[,1])
freqWords <- findFreqTerms(tdmSamples, 2)
length(freqWords)
n <- 25L # variable to set top n words
dfTop <- data.frame(words = names(tdmMatrixSorted[1:n]), frequency = tdmMatrixSorted[1:n])
dfTop$words <- reorder(dfTop$words, dfTop$frequency)
g.top <- ggplot(dfTop, aes(x = words, y = frequency))
g.top <- g.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent")
g.top
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
dtmSamplesBigram <- DocumentTermMatrix(dtmSamples, control = list(tokenize = BigramTokenizer))
tdmSamplesBigram <- DocumentTermMatrix(tdmSamples, control = list(tokenize = BigramTokenizer))
docsSamples
dtmSamplesBigram <- DocumentTermMatrix(dtmSamples, control = list(tokenize = BigramTokenizer))
docsSamples <- Corpus(VectorSource(c(blogsSample, newsSample, twitterSample)), readerControl = list(readPlain,language="en_US", load = TRUE))
dtmSamplesBigram <- DocumentTermMatrix(dtmSamples, control = list(tokenize = BigramTokenizer))
docsSamples[[1]]
docsSamples[[1]]$content
docsSamples
dtmSamplesBigram <- DocumentTermMatrix(tdmSamples, control = list(tokenize = BigramTokenizer))
dtmSamplesBigram <- TermDocumentMatrix(tdmSamples, control = list(tokenize = BigramTokenizer))
dtmSamplesBigram <- DocumentTermMatrix(docsSamples, control = list(tokenize = BigramTokenizer))
dtmSamplesTrigram <- DocumentTermMatrix(docsSamples, control = list(tokenize = TrigramTokenizer))
dtmSamplesBigram
dtmSamplesBigram$dimnames$Terms
dtmSamplesTrigram$dimnames$Terms
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
dtmSamplesBigram <- DocumentTermMatrix(docsSamples, control = list(tokenize = BigramTokenizer))
dtmSamplesTrigram <- DocumentTermMatrix(docsSamples, control = list(tokenize = TrigramTokenizer))
dtmSamplesBigram$i
dtmSamplesBigram$nrow
dtmSamplesBigram$ncol
dtmSamplesBigram
dtmSamplesBigram$dimnames$Docs
dtmSamplesBigram$dimnames$Terms
docsSamples <- VCorpus(VectorSource(c(blogsSample, newsSample, twitterSample)), readerControl = list(readPlain,language="en_US", load = TRUE))
# Data frame to store counts
dfSample <- data.frame(text.source = c("blogs_Sample", "twitter_Sample", "news_Sample"), line.count = NA, word.count = NA)
# store the files into a list
my.list <- list(blogs_Sample = blogsSample, twitter_sample = twitterSample, news_Sample = newsSample)
# Counts lines and words
dfSample$line.count <- sapply(my.list, length)
dfSample$word.count <- sapply(my.list, f.word.count)
g.line.count <- ggplot(dfSample, aes(x = factor(text.source), y = line.count/1e+03))
g.line.count <- g.line.count + geom_bar(stat = "identity") +
labs(y = "# of lines/thousands", x = "text source", title = "Count of lines per Sample Corpus")
g.word.count <- ggplot(dfSample, aes(x = factor(text.source), y = word.count/1e+03))
g.word.count <- g.word.count + geom_bar(stat = "identity") +
labs(y = "# of words/thousands", x = "text source", title = "Count of words per Sample Corpus")
g.line.count
g.word.count
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
dtmSamplesBigram <- DocumentTermMatrix(docsSamples, control = list(tokenize = BigramTokenizer))
dtmSamplesTrigram <- DocumentTermMatrix(docsSamples, control = list(tokenize = TrigramTokenizer))
dtmSamplesBigram$dimnames$Terms
dtmSamplesTrigram$dimnames$Terms
inspect(dtmSamplesBigram)
inspect(dtmSamplesTrigram)
inspect(dtmSamplesBigram)
dtmSamplesBigramMatrix <- as.matrix(dtmSamplesBigram)
dtmSamplesBigramMatrix <- as.matrix(dtmSamplesBigram)
dtmSamplesBigramMatrixSorted <- sort(rowSums(dtmSamplesBigram), decreasing = TRUE)
dtmSamplesBigramMatrixSorted <- sort(colSums(dtmSamplesBigram), decreasing = TRUE)
tdmSamplesBigram <- TermDocumentMatrix(docsSamples, control = list(tokenize = BigramTokenizer))
tdmSamplesBigram <- TermDocumentMatrix(docsSamples, control = list(tokenize = BigramTokenizer))
#library(testthat)
#library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
library(ggplot2)
library(NLP)
library(openNLP)
library(RWeka)
library(tm)
set.seed(43)
Sys.setlocale("LC_ALL", "American")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogsFile <- unz(destiny, 'final/en_US/en_US.blogs.txt')
blogs <- readLines(blogsFile, skipNul = TRUE)
close(blogsFile)
newsFile <- unz(destiny, 'final/en_US/en_US.news.txt')
news <- readLines(newsFile, skipNul = TRUE)
close(newsFile)
twitterFile <- unz(destiny, 'final/en_US/en_US.twitter.txt')
twitter <- readLines(twitterFile, skipNul = TRUE)
close(twitterFile)
profanityList <- lexicon::profanity_banned # Banned words
tdmSamplesBigram <- TermDocumentMatrix(docsSamples, control = list(tokenize = BigramTokenizer))
tdmSamplesBigramMatrix <- as.matrix(tdmSamplesBigram)
rm(dfOrg)
rm(dfSample)
rm(dfTop)
tdmSamplesBigramMatrix <- as.matrix(tdmSamplesBigram)
rm(dtmSamples)
rm(dtmSamplesBigram)
rm(dtmSamplesBigramMatrix)
rm(dtmSamplesTrigram)
tdmSamplesBigramMatrix <- as.matrix(tdmSamplesBigram)
rm(blogs)
rm(news)
rm(twitter)
tdmSamplesBigramMatrix <- as.matrix(tdmSamplesBigram)
rm(tdmMatrixSorted)
tdmSamplesBigramMatrix <- as.matrix(tdmSamplesBigram)
rm(blogsSample)
rm(freqWords)
rm(newsSample)
rm(twitterSample)
tdmSamplesBigramMatrix <- as.matrix(tdmSamplesBigram)
tdmSamplesBigram <- removeSparseTerms(tdmSamplesBigram, 0.99)
library(stopwords)
library(sentimentr)
library(dplyr)
library(ggplot2)
library(NLP)
library(openNLP)
library(RWeka)
library(tm)
library(tidytext)
set.seed(43)
tdmSamplesBigram <- removeSparseTerms(tdmSamplesBigram, 0.99)
#dtmSamplesBigram <- DocumentTermMatrix(docsSamples, control = list(tokenize = BigramTokenizer))
#dtmSamplesBigram.commn <- removeSparseTerms(dtmSamplesBigram, 0.1)
tdmSamplesBigramMatrix <- as.matrix(tdmSamplesBigram)
#DF <- tidytext::tidy(dtmSamplesBigram)
tdmSamplesBigramSorted <- sort(rowSums(tdmSamplesBigramMatrix), decreasing = TRUE)
View(tdmSamplesBigramSorted)
>>>>>>> Stashed changes
