expect_error(object = "./data/Coursera_swiftKey.zip" )
})
test_that("passing .... then an error is thrown",{
expect_match(object = destinyDirectory + destinyFile, regexp = "./data/Coursera_swiftKey.zip")
})
createDestination <- function(destinyDirectory){
if (!file.exists(destinyDirectory)){
dir.create(destinyDirectory)
}
return !file.exists(destinyDirectory)
}
source('C:/Users/hongji/GitHub/Capstone/createDestination.R')
source('C:/Users/hongji/GitHub/Capstone/createDestination.R')
library(testthat)
context("Obtaining the data")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
test_that("passing .... then an error is thrown",{
expect_equal(createDestination(destinyDirectory),TRUE)
})
test_that("passing .... then an error is thrown",{
expect_equal(createDestination(destinyDirectory),TRUE)
})
test_that("passing .... then an error is thrown",
{
expect_equal(createDestination(destinyDirectory),TRUE)
}
)
expect_true(createDestination(destinyDirectory))
test_that("Test the directory exists",
{
expect_true(createDestination(destinyDirectory))
}
)
auto_test(createDestination(destinyDirectory))
source('C:/Users/hongji/GitHub/Capstone/downloadData.R')
auto_test(downloadData(dataURL, destinyDirectory, destinyFile))
source('C:/Users/hongji/GitHub/Capstone/downloadData.R')
auto_test(createDestination(destinyDirectory))
auto_test(createDestination(destinyDirectory)),
auto_test(downloadData(dataURL, destinyDirectory, destinyFile))
source('C:/Users/hongji/GitHub/Capstone/downloadData.R')
library(testthat)
context("Obtaining the data")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
test_that("Test the directory exists",
{
auto_test(createDestination(destinyDirectory))
auto_test(downloadData(dataURL, destinyDirectory, destinyFile))
}
)
library(testthat)
context("Obtaining the data")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
test_that("Test the directory exists",
{
expect_true(createDestination(destinyDirectory))
expect_true(downloadData(dataURL, destinyDirectory, destinyFile))
}
)
source('C:/Users/hongji/GitHub/Capstone/downloadData.R')
library(testthat)
context("Obtaining the data")
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
test_that("Test the directory exists",
{
expect_true(createDestination(destinyDirectory))
expect_true(downloadData(dataURL, destinyDirectory, destinyFile))
}
)
install.packages("tokenizers")
wholeData <- paste(blogs, news, twitter)
wholeData <- paste(blogs, news, twitter)
library(testthat)
set.seed(43)
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), samples)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), samples)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), samples)
wholeData <- paste(blogs, news, twitter)
samples <- sample(c(1:10000), 1000)
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), samples)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), samples)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), samples)
wholeData <- paste(blogs, news, twitter)
tail(wholeData)
words <- tokenize_words(wholeData, stopwords = stopwords::stopwords("en"))
install.packages("stopwords")
words <- tokenize_words(wholeData, stopwords = stopwords::stopwords("en"))
words <- tokenize_words(wholeData, stopwords = stopwords::stopwords("en"))
library(testthat)
library(tokenizers)
library(stopwords)
words <- tokenize_words(wholeData, stopwords = stopwords::stopwords("en"))
tail(words)
ngrams <- tokenize_ngrams(wholeData, n =5, n_min = 2, stopwords = stopwords::stopwords("en"))
str(words)
str(ngrams)
install.packages("sentimentr")
library(sentimentr)
profanity(words, profanity_list = lexicon::profanity_alvarez)
apply(words, profanity, profanity_list = lexicon::profanity_alvarez)
apply(words, profanity(), profanity_list = lexicon::profanity_alvarez)
apply(words, FUN = profanity)
profanity(words[1], profanity_list = lexicon::profanity_alvarez)
profanity(as.character(words[1]) , profanity_list = lexicon::profanity_alvarez)
profanity(wholeData , profanity_list = lexicon::profanity_alvarez)
profanity(get_sentences(wholeData)  , profanity_list = lexicon::profanity_alvarez)
wordsProfanity <- profanity(get_sentences(wholeData)  , profanity_list = lexicon::profanity_alvarez)
wordsProfanity
lexicon::profanity_alvarez
lexicon::profanity_arr_bad
library(dplyr)
words[words %in% lexicon::profanity_alvarez]
words[!(words %in% lexicon::profanity_alvarez)]
words <- words[!(words %in% lexicon::profanity_alvarez)]
words <- wordsStem[!(wordsStem %in% lexicon::profanity_alvarez)]
words <- ngrams[!(ngrams %in% lexicon::profanity_alvarez)]
library(testthat)
library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
set.seed(43)
## Data Load
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
samples <- sample(c(1:10000), 1000)
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), samples)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), samples)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), samples)
wholeData <- paste(blogs, news, twitter)
## Tokenizing through tokenizers package
words <- tokenize_words(wholeData, stopwords = stopwords::stopwords("en"))
wordsStem <- tokenize_word_stems()
ngrams <- tokenize_ngrams(wholeData, n =5, n_min = 2, stopwords = stopwords::stopwords("en"))
## Profanity filter
#wordsProfanity <- profanity(get_sentences(wholeData)  , profanity_list = lexicon::profanity_alvarez)
words <- words[!(words %in% lexicon::profanity_alvarez)]
wordsStem <- wordsStem[!(wordsStem %in% lexicon::profanity_alvarez)]
ngrams <- ngrams[!(ngrams %in% lexicon::profanity_alvarez)]
library(testthat)
library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
set.seed(43)
## Data Load
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
samples <- sample(c(1:10000), 1000)
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'), samples)
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'), samples)
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'), samples)
wholeData <- paste(blogs, news, twitter)
## Tokenizing through tokenizers package
words <- tokenize_words(wholeData, stopwords = stopwords::stopwords("en"))
wordsStem <- tokenize_word_stems(wholeData, stopwords = stopwords::stopwords("en"))
ngrams <- tokenize_ngrams(wholeData, n =5, n_min = 2, stopwords = stopwords::stopwords("en"))
## Profanity filter
#wordsProfanity <- profanity(get_sentences(wholeData)  , profanity_list = lexicon::profanity_alvarez)
words <- words[!(words %in% lexicon::profanity_alvarez)]
wordsStem <- wordsStem[!(wordsStem %in% lexicon::profanity_alvarez)]
ngrams <- ngrams[!(ngrams %in% lexicon::profanity_alvarez)]
ngrams
append(words, 'pussy')
tail(words)
words[words == 'pussy']
words <- tokenize_words(wholeData, stopwords = stopwords::stopwords("en"))
append(words[1,], 'pussy')
append(words[1,1], 'pussy')
append(words[,100], 'pussy')
append(words[100], 'pussy')
words <- words[!(words %in% lexicon::profanity_alvarez)]
words[100]
append(words[100], 'pussy')
words[100]
append(words[100,100], 'pussy')
append(words[100,1], 'pussy')
append(words[100,], 'pussy')
append(words[100], 'pussy')
append(words[1000], 'pussy')
words[100][2]
words[100,2]
words[100,1]
words[100]
words[101]
View(words)
append(words[[1000]], 'pussy')
words[[1000]]
append(words[[1000,1]], 'pussy')
append(words[[1000][1]], 'pussy')
append(words[[1000,1]], 'pussy')
append(words[[1000],1], 'pussy')
append(words[[1000]][1], 'pussy')
words[[1000]][1]
words[[1000]][2]
words[[1000]][3]
words[[1000]][4]
words[[1000]][5]
words[[1000]][-1]
append(words[[1000]][1], 'pussy')
words[[1000]][-1]
append(words[[1]], 'pussy')
words[[1]] <- append(words[[1]], 'pussy')
words[[1]]
words <- words[!(words %in% lexicon::profanity_alvarez)]
words[[1]]
words <- words[!(lexicon::profanity_alvarez %in% words )]
words[[1]]
words <- tokenize_words(wholeData, stopwords = merge(stopwords::stopwords("en"), lexicon::profanity_alvarez))
summary(wholeData)
help("rbinom")
rbinom(length(blogs)*0.1, length(blogs), 0.5)
rbinom(length(blogs)*0.1, length(blogs), 0.5)
rbinom(length(blogs)*0.1, length(blogs), 0.5)
rbinom(length(blogs)*0.1, length(blogs), 0.5)
blogs[rbinom(length(blogs), length(blogs), 0.1)]
rbinom(length(blogs), length(blogs), 0.1)
tail(rbinom(length(blogs), length(blogs),0.1))
head(rbinom(length(blogs), length(blogs),0.1))
if (!file.exists('./data/sample')){
dir.create('./data/sample')
} else {
print("the directory already exists")
}
return(file.exists('./data/sample'))
if (!file.exists('./data/sample')){
dir.create('./data/sample')
} else {
print("the directory already exists")
}
library(testthat)
library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
set.seed(43)
## Data Load
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
## Loading Data
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'))
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'))
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'))
### Sampling
if (!file.exists('./data/sample')){
dir.create('./data/sample')
} else {
print("the directory already exists")
}
blogsSample <- blogs[rbinom(length(blogs)*.1, length(blogs), 0.1)]
newsSample <- blogs[rbinom(length(newsSample)*.1, length(newsSample), 0.1)]
twitterSample <- blogs[rbinom(length(twitterSample)*.1, length(twitterSample), 0.1)]
newsSample <- blogs[rbinom(length(news)*.1, length(news), 0.1)]
twitterSample <- blogs[rbinom(length(twitter)*.1, length(twitter), 0.1)]
write.table(blogsSample, file = './sample/en_US.blogsSample.csv', col.names = FALSE, sep = ".")
write.table(newsSample, file = './sample/en_US.newsSample.csv', col.names = FALSE, sep = ".")
write.table(twitterSample, file = './sample/en_US.twitterSample.csv', col.names = FALSE, sep = ".")
write.table(blogsSample, file = "./sample/en_US.blogsSample.csv", col.names = FALSE, sep = ".")
write.table(blogsSample, file = './sample/en_US.blogsSample.csv', col.names = FALSE, sep = ".", "aw")
write.table(blogsSample, file = './sample/en_US.blogsSample.csv', col.names = FALSE, sep = ".", "w")
write.table(blogsSample, file = './sample/en_US.blogsSample.csv', col.names = FALSE, sep = ".", "a")
write.table(blogsSample, file = './data/sample/en_US.blogsSample.csv', col.names = FALSE, sep = ".")
write.table(newsSample, file = './data/sample/en_US.newsSample.csv', col.names = FALSE, sep = ".")
write.table(twitterSample, file = './data/sample/en_US.twitterSample.csv', col.names = FALSE, sep = ".")
install.packages("corpus")
help(corpus)
library(corpus)
help("corpus")
summary(twitter)
str(twitter)
summarise(twitter)
summary(twitter)
summary(news)
summary(blogs)
type(news)
summarise_(twitter)
length(twitter)
length(twitter[])
length(twitter[1])
length(twitter[2])
length(twitter[5])
length(twitter[10])
length(twitter[1][1])
length(as.character(twitter[1][1]))
head(twitter)
length(lenth(twitter[10]))
length(length(twitter[10]))
length(as.charater(twitter[10]))
length(as.character(twitter[10]))
data.frame(blogsSample)
blogs['love' %in% blogs]
lenn <- nchar(blogs)
max(lenn)
lennb <- nchar(blogs)
lennn <- nchar(news)
lennt <- nchar(twitter)
max(lennb, lennn, lennt)
max(lennn)
max(lennt)
lovec <- grepl(".love.", twitter, ignore.case = FALSE)
hatec <- grepl(".hate.", twitter, ignore.case = FALSE)
lovec
lovec[lovec == TRUE]
mean(lovec)
mean(lovec) / mean(hatec)
lovec <- grepl(".love.", twitter, ignore.case = FALSE)
twitter[grepl(".biostats.", twitter, ignore.case = FALSE)]
lovec <- grepl(".love.", twitter, ignore.case = FALSE)
twitter[grepl("A computer once beat me at chess, but it was no match for me at kickboxing", twitter, ignore.case = FALSE)]
lovec <- grepl(".love.", twitter, ignore.case = FALSE)
twitter[grepl(".biostats.", twitter, ignore.case = FALSE)]
lovec <- grepl(".love.", twitter, ignore.case = FALSE)
twitter[grepl("A computer once beat me at chess, but it was no match for me at kickboxing", twitter, ignore.case = FALSE)]
install.packages("corpus")
library(testthat)
library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
library(corpus)
set.seed(43)
# Getting and cleaning data
## Data Load
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
## Loading Data
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'))
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'))
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'))
### Sampling
if (!file.exists('./data/sample')){
dir.create('./data/sample')
} else {
print("the directory already exists")
}
blogsSample <- blogs[rbinom(length(blogs)*.1, length(blogs), 0.1)]
newsSample <- blogs[rbinom(length(news)*.1, length(news), 0.1)]
twitterSample <- blogs[rbinom(length(twitter)*.1, length(twitter), 0.1)]
newsSample
grepl(lexicon::profanity_alvarez, twitter, ignore.case = FALSE)
twitter[grepl(lexicon::profanity_alvarez, twitter, ignore.case = FALSE)==TRUE]
install.packages("SnowballC")
reut21578XMLgz <- system.file("texts", "reut21578.xml.gz", package = "tm")
tmMap(blogsSample, asPlain)
library(tm)
tmMap(blogsSample, asPlain)
tm_map(blogsSample, asPlain)
tm_map(blogsSample, stripWhitespace)
blogsSample <- blogs[rbinom(length(blogs)*.1, length(blogs), 0.1)]
library(testthat)
library(tokenizers)
library(stopwords)
library(sentimentr)
library(dplyr)
library(corpus)
library(RWeka)
library(tm)
set.seed(43)
# Getting and cleaning data
## Data Load
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'))
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'))
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'))
blogsSample <- blogs[rbinom(length(blogs)*.1, length(blogs), 0.1)]
newsSample <- blogs[rbinom(length(news)*.1, length(news), 0.1)]
twitterSample <- blogs[rbinom(length(twitter)*.1, length(twitter), 0.1)]
blogsSample <- blogs[rbinom(length(blogs)*.1, length(blogs), 0.5)]
newsSample <- blogs[rbinom(length(news)*.1, length(news), 0.5)]
Corpus(blogs)
new
install.packages("RColorBrewer")
blogsText <- Corpus(VectorSource(blogs))
blogsText
blogsText <- tm_map(blogsText, stripWhitespace)
blogsText
blogsText <- tm_map(blogsText, tolower)
blogsText <- VCorpus(VectorSource(blogs))
View(blogsText)
View(blogsSample)
blogsText[[1]]
blogsText[[2]]
library(testthat)
#library(tokenizers)
library(stopwords)
#library(sentimentr)
library(dplyr)
library(corpus)
library(RWeka)
library(tm)
set.seed(43)
# Getting and cleaning data
## Data Load
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
## Loading Data
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'))
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'))
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'))
### Sampling
if (!file.exists('./data/sample')){
dir.create('./data/sample')
} else {
print("the directory already exists")
}
blogsSample <- blogs[rbinom(length(blogs)*.1, length(blogs), 0.1)]
newsSample <- blogs[rbinom(length(news)*.1, length(news), 0.1)]
twitterSample <- blogs[rbinom(length(twitter)*.1, length(twitter), 0.1)]
### Write samples into files
writeLines(blogsSample, file = './data/sample/en_US.blogsSample.csv')
writeLines(newsSample, file = './data/sample/en_US.newsSample.csv')
writeLines(twitterSample, file = './data/sample/en_US.twitterSample.csv')
writeLines(blogsSample, con = './data/sample/en_US.blogsSample.txt')
library(testthat)
#library(tokenizers)
library(stopwords)
#library(sentimentr)
library(dplyr)
library(corpus)
library(RWeka)
library(tm)
set.seed(43)
# Getting and cleaning data
## Data Load
destinyDirectory = "./data/"
destinyFile = "Coursera_SwiftKey.zip"
destiny = paste0(destinyDirectory, destinyFile)
dataURL = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
## Loading Data
blogs <- readLines(unz(destiny, 'final/en_US/en_US.blogs.txt'))
news <- readLines(unz(destiny, 'final/en_US/en_US.news.txt'))
twitter <- readLines(unz(destiny, 'final/en_US/en_US.twitter.txt'))
### Sampling
if (!file.exists('./data/sample')){
dir.create('./data/sample')
} else {
print("the directory already exists")
}
blogsSample <- blogs[rbinom(length(blogs)*.1, length(blogs), 0.1)]
newsSample <- blogs[rbinom(length(news)*.1, length(news), 0.1)]
twitterSample <- blogs[rbinom(length(twitter)*.1, length(twitter), 0.1)]
### Write samples into files
writeLines(blogsSample, con = './data/sample/en_US.blogsSample.txt')
writeLines(newsSample, con = './data/sample/en_US.newsSample.txt')
writeLines(twitterSample, con = './data/sample/en_US.twitterSample.txt')
rm(list = ls())
blogsSample <- readLines(file = './data/sample/en_US.blogsSample.txt')
newsSample <- readLines(file = './data/sample/en_US.newsSample.txt')
twitterSample <- readLines(file = './data/sample/en_US.twitterSample.txt')
blogsSample <- readLines(con = './data/sample/en_US.blogsSample.txt')
newsSample <- readLines(con = './data/sample/en_US.newsSample.txt')
twitterSample <- readLines(con = './data/sample/en_US.twitterSample.txt')
text <- Corpus(VectorSource(blogsSample, newsSample, twitterSample))
texts <- append(texts, blogsSample)
texts <- append(texts, newsSample)
texts <- append(texts, twitterSample)
texts <- append(blogsSample, newsSample)
texts <- append(texts, twitterSample)
docs <- Corpus(VectorSource(texts))
docs <- VCorpus(VectorSource(texts))
docs <- tm_map(docs, stripWhitespace)
View(docs)
View(texts)
library(sentimentr)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, lexicon::profanity_alvarez)
docs <- tm_map(docs, removeWords, unlist(lexicon::profanity_alvarez))
docs <- tm_map(docs, removeWords, unlist(lexicon::profanity_alvarez))
lexicon::profanity_alvarez
docs <- tm_map(docs, removeWords, regexpr(lexicon::profanity_alvarez))
docs <- tm_map(docs, removeWords, profanity)
profanityList <- lexicon::profanity_alvarez
writeLines(profanityList, con = './data/sample/profanityList.txt')
profanityList <- readline(con = './data/sample/profanityList.txt')
profanityList <- readLines(con = './data/sample/profanityList.txt')
texts <- append(blogsSample, newsSample)
texts <- append(texts, twitterSample)
docs <- tm_map(docs, removeWords, profanity)
docs <- tm_map(docs, removeWords, list(profanity))
docs <- tm_map(docs, removeWords, unlist(profanity))
docs <- tm_map(docs, removeWords, profanityList)
docs <- tm_map(docs, removeWords, grepl(profanityList))
docs <- tm_map(docs, removeWords, profanityList)
